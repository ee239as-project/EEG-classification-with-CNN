{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for issue loading Utils.preprocess_util\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "# Variable: wrapper for PyTorch tensor, stores gradients with requires_grad=True\n",
    "# x.data, x.grad.data: values of tensor x, gradient values of x\n",
    "from torch.autograd import Variable\n",
    "from Utils.preprocess_util import *\n",
    "from Utils.visualize import *\n",
    "\n",
    "\n",
    "# ----------------------------------- PREPROCESSING -----------------------------------\n",
    "X_train,X_valid,X_test, Y_train,Y_valid,Y_test = load_preprocess_eeg_data()\n",
    "'''\n",
    "Training data: (177125, 22, 500)\n",
    "Training target: (177125,)\n",
    "Validation data: (87250, 22, 500)\n",
    "Validation target: (87250,)\n",
    "Test data: (55375, 22, 500)\n",
    "Test target: (55375,)\n",
    "'''\n",
    "\n",
    "# CPU datatype: change to torch.cuda.FloatTensor to use GPU\n",
    "dtype = torch.FloatTensor\n",
    "x = Variable(torch.tensor(X_train))\n",
    "y = Variable(torch.tensor(Y_train), requires_grad=False)\n",
    "x.type(dtype)\n",
    "y.type(dtype)\n",
    "\n",
    "# ------------------------------------- CNN MODEL -------------------------------------\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # example x.size: ([125, 64, 40])\n",
    "        a = x.view(x.size(0), -1)\n",
    "        return a\n",
    "\n",
    "class threed_to_twod(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # example x.shape: ([125, 40, 1, 450])\n",
    "        a = x.reshape(x.shape[0], x.shape[3], x.shape[1])\n",
    "        # example a.shape: ([125, 450, 40])\n",
    "        return a\n",
    "\n",
    "dropout = 0\n",
    "model = nn.Sequential()\n",
    "model.add_module('conv_across_time', nn.Conv2d(1, 40, kernel_size=(1,51) ,stride=1))\n",
    "model.add_module('conv_across_electrodes', nn.Conv2d(40, 40, kernel_size=(22,1), stride=1))\n",
    "model.add_module('BatchNorm2d', nn.BatchNorm2d(40, momentum=0.1))\n",
    "model.add_module('Nonlinearity', nn.ReLU(inplace=True))\n",
    "model.add_module('correct_dimensions', threed_to_twod())\n",
    "model.add_module('AvgPool2d', nn.AvgPool2d(kernel_size=(135,1), stride=(5,1)))\n",
    "model.add_module('drop', nn.Dropout(p=dropout))\n",
    "model.add_module('Flatten', Flatten())\n",
    "model.add_module('Fc_layer', nn.Linear(2560,10))\n",
    "torch.nn.init.xavier_uniform_(model.conv_across_time.weight, gain=1)\n",
    "torch.nn.init.xavier_uniform_(model.conv_across_electrodes.weight, gain=1)\n",
    "\n",
    "model.type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "\n",
    "# ---------------------------------- HYPERPARAMETERS ----------------------------------\n",
    "lr = 1e-4\n",
    "betas = (0.9, 0.999)\n",
    "eps = 1e-8\n",
    "wt_dcy = 0\n",
    "amsgrad = False\n",
    "'''\n",
    "wt_scale = 0.01\n",
    "reg = 0.001\n",
    "lr_decay = None\n",
    "'''\n",
    "params = model.parameters()\n",
    "optimizer = optim.Adam(params, lr=lr, betas=betas, eps=eps, weight_decay=wt_dcy, amsgrad=amsgrad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- TRAINING -------------------------------------\n",
    "'''\n",
    "for t in range(3):\n",
    "    # calculate loss\n",
    "    y_pred = model(x.float())\n",
    "    loss = loss_fn(y_pred, y.type(torch.LongTensor))\n",
    "\n",
    "    # backprop\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # update the parameters\n",
    "    optimizer.step()\n",
    "y_pred = model(x)\n",
    "loss = loss_fn(y_pred, y.type(torch.LongTensor))\n",
    "print(loss)\n",
    "print(model)\n",
    "'''\n",
    "\n",
    "n_train = X_train.shape[0] # 177125\n",
    "n_validation = X_valid.shape[0] # 87250\n",
    "batch_size = 125\n",
    "iter_per_epoch = max(n_train // batch_size, 1) # 1417\n",
    "\n",
    "n_epochs = 1 # Change back to 50\n",
    "n_iter = n_epochs * iter_per_epoch # 708500\n",
    "\n",
    "# Book-keeping\n",
    "best_val_acc = 0\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "epoch = 0\n",
    "for t in range(n_iter):\n",
    "    # Make a minibatch of training data\n",
    "    batch_mask = np.random.choice(n_train, batch_size)\n",
    "    X_batch_tensor = threeD_to_fourDTensor(X_train[batch_mask]).float() # (125, 1, 22, 500)\n",
    "    y_batch_tensor = Variable(torch.tensor(Y_train[batch_mask])).type(torch.LongTensor) # (125)\n",
    "\n",
    "    # Compute loss and gradient\n",
    "    y_pred = model(X_batch_tensor)\n",
    "    loss = loss_fn(y_pred, y_batch_tensor)\n",
    "    loss_history.append(loss.data)\n",
    "\n",
    "    # Perform a parameter update\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print training loss\n",
    "    if (t%10 == 0):\n",
    "        print('(Iteration %d / %d) loss: %f' % (t + 1, n_iter, loss_history[-1]))\n",
    "\n",
    "    # At end of every epoch, increment epoch counters and consider decaying learning rate\n",
    "    epoch_end = (t+1) % iter_per_epoch == 0\n",
    "    if epoch_end:\n",
    "        epoch += 1\n",
    "\n",
    "    # Check train and val accuracy on first iteration, last iteration, and at end of each epoch\n",
    "    first_iter = (t == 0)\n",
    "    last_iter = (t == n_iter - 1)\n",
    "\n",
    "    if first_iter or last_iter or epoch_end:\n",
    "        train_acc = check_accuracy(model, X_train, Y_train, n_train)\n",
    "        val_acc = check_accuracy(model, X_valid, Y_valid, n_validation)\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_acc_history.append(val_acc)\n",
    "\n",
    "        best_val_acc = max(best_val_acc, val_acc)\n",
    "        save_checkpoint(epoch, loss_history, train_acc_history, val_acc_history, best_val_acc)\n",
    "\n",
    "        print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (epoch, n_epochs, train_acc, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- PLOTTING -------------------------------------\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "\n",
    "# Training loss over iterations\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(loss_history, 'o')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Training and validation accuracy over epochs\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(train_acc_history, '-o')\n",
    "plt.plot(val_acc_history, '-o')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
