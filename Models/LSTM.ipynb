{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for issue loading Utils.preprocess_util\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "sys.path.append('/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages')\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from Utils.preprocess_util import *\n",
    "from Utils.visualize import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2115, 1)\n",
      "[0. 1. 2. 3. 4. 5. 6. 7. 8.]\n"
     ]
    }
   ],
   "source": [
    "person_train_valid = np.load('../Data/person_train_valid.npy')\n",
    "print(person_train_valid.shape)\n",
    "print(np.unique(person_train_valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41]\n",
      " [43]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([1,2,4,5,1,4,1,5])\n",
    "c = np.argwhere(a==4)\n",
    "\n",
    "b = np.array([11,21,41,15,11,43,13,56])\n",
    "print(b[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cropping:\n",
      "Training data: (177125, 22, 500)\n",
      "Training target: (177125,)\n",
      "Validation data: (87250, 22, 500)\n",
      "Validation target: (87250,)\n",
      "Test data: (55375, 22, 500)\n",
      "Test target: (55375,)\n",
      "Person train/validation: (2115, 1)\n",
      "Person test: (443, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_valid,X_test,Y_train,Y_valid,Y_test = load_preprocess_eeg_data(person=None,crop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(X_train.shape[0], X_train.shape[0], replace=False)\n",
    "X_train = X_train[indices]\n",
    "Y_train = Y_train[indices]\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_small = X_train[0:1000]\n",
    "Y_train_small = Y_train[0:1000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(X_valid.shape[0], X_valid.shape[0], replace=False)\n",
    "X_valid = X_valid[indices]\n",
    "Y_valid = Y_valid[indices]\n",
    "\n",
    "indices = np.random.choice(X_test.shape[0], X_test.shape[0], replace=False)\n",
    "X_test = X_test[indices]\n",
    "Y_test = Y_test[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature and targets tensor for train set\n",
    "features_train = torch.from_numpy(X_train)\n",
    "targets_train = torch.from_numpy(Y_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set\n",
    "features_test = torch.from_numpy(X_test)\n",
    "targets_test = torch.from_numpy(Y_test).type(torch.LongTensor)\n",
    "\n",
    "features_valid = torch.from_numpy(X_valid)\n",
    "targets_valid = torch.from_numpy(Y_valid).type(torch.LongTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-45dfe172c61f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdroput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTMModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_neurons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, batch_size, n_steps, n_inputs, n_neurons, n_outputs,n_layers,droput):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.n_neurons = n_neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.n_steps = n_steps\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.num_layers = n_layers\n",
    "        self.lstm = nn.LSTM(self.n_inputs, self.n_neurons,self.num_layers) \n",
    "        #self.lstm.weight_hh_l0.data.fill_(0)\n",
    "        #torch.nn.init.xavier_uniform_(self.lstm.weight_ih_l0.data )\n",
    "        #torch.nn.init.orthogonal_(self.lstm.weight_hh_l0.data)\n",
    "        \n",
    "        #initialising w(rec) to I and b(rec) to 0 \n",
    "        ih_size = list(self.lstm.weight_ih_l0.data.shape)\n",
    "        hh_size =list(self.lstm.weight_hh_l0.data.shape)\n",
    "        self.lstm.weight_ih_l0.data.copy_(torch.eye(ih_size[0],ih_size[1]))\n",
    "        self.lstm.weight_hh_l0.data.copy_(torch.eye(hh_size[0],hh_size[1]))\n",
    "        \n",
    "        self.lstm.bias_ih_l0.data.fill_(0)\n",
    "        self.lstm.bias_hh_l0.data.fill_(0)\n",
    "        \n",
    "        self.droput = nn.Dropout(p=droput)\n",
    "        self.FC = nn.Linear(self.n_neurons, self.n_outputs)\n",
    "        \n",
    "    def init_hidden(self,):\n",
    "            # (num_layers, batch_size, n_neurons)\n",
    "            return (torch.zeros(self.num_layers, self.batch_size, self.n_neurons))\n",
    "            #return torch.nn.init.xavier_uniform_((self.num_layers, self.batch_size, self.n_neurons), gain=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "            # transforms X to (n_steps, batch_size, n_inputs)\n",
    "            X = X.permute(1, 0, 2) \n",
    "            self.batch_size = X.size(1)\n",
    "            self.hidden = self.init_hidden()\n",
    "            self.cellstate = self.init_hidden()\n",
    "            lstm_out, (self.hidden, self.cellstate)= self.lstm(X, (self.hidden,self.cellstate))\n",
    "            hidden_out =self.hidden[self.num_layers-1]\n",
    "            dropout_out = self.droput(hidden_out)\n",
    "            out = self.FC(dropout_out)\n",
    "\n",
    "            return out.view(-1, self.n_outputs) # (batch_size, n_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(features_train, targets_train)\n",
    "valid = torch.utils.data.TensorDataset(features_valid, targets_valid)\n",
    "test = torch.utils.data.TensorDataset(features_test, targets_test)\n",
    "\n",
    "# data loader\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False)\n",
    "valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# pprint.pprint(test_loader.dataset.tensors[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STEPS = 500\n",
    "N_INPUTS = 22\n",
    "N_NEURONS = 75\n",
    "N_OUTPUTS = 10\n",
    "N_EPOCHS = 10\n",
    "N_LAYERS = 1# This actually corresponds to how many lsts are stacked one above the other\n",
    "droput = 0\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "model = LSTMModel(batch_size, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS,N_LAYERS,droput)\n",
    "\n",
    "# (batch_size, n_steps, n_inputs)\n",
    "images_modified = images.view(-1, 500, 22)\n",
    "logits = model(images_modified.float())\n",
    "print(logits[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "n_iters = 10000\n",
    "num_epochs = int(n_iters / (len(X_train)/batch_size))\n",
    "model = LSTMModel(batch_size, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS,N_LAYERS,droput)\n",
    "\n",
    "# Cross Entropy Loss \n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n",
    "\n",
    "# batch GD\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.000001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01,weight_decay =0,betas=(0.9, 0.999),amsgrad=False)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(),lr=0.001, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0.1)\n",
    "\n",
    "# Create RNN\n",
    "input_dim = 22\n",
    "seq_dim = 500\n",
    "\n",
    "train_loss = []\n",
    "iterations = []\n",
    "train_acc = []\n",
    "\n",
    "X_valid_tensor = torch.from_numpy(X_valid.reshape(-1, seq_dim, input_dim))\n",
    "X_train_tensor = torch.from_numpy(X_train.reshape(-1, seq_dim, input_dim))\n",
    "\n",
    "print(\"X=\",X_train_tensor.shape)\n",
    "print(\"num_epochs = \", num_epochs)\n",
    "print(\"n_iters = \", n_iters)\n",
    "print(\"starting training..\")\n",
    "\n",
    "count = 0\n",
    "num_epochs = 2\n",
    "print(\"starting training..\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"epoch=\",epoch)\n",
    "    # reset hidden states\n",
    "    \n",
    "    for i, (signals, labels) in enumerate(train_loader):\n",
    "        train  = Variable(signals.view(-1, seq_dim, input_dim))\n",
    "        labels = Variable(labels )\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # reset hidden states\n",
    "        model.hidden = model.init_hidden() \n",
    "                \n",
    "        # Forward propagation\n",
    "        outputs = model(train.float())\n",
    "        \n",
    "        # Calculate softmax and cross entropy loss\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "                    \n",
    "        #print(\"parameters===\",list(model.parameters())[0].data)\n",
    "\n",
    "        count += 1\n",
    "        train_loss.append(loss.data)\n",
    "        iterations.append(count)\n",
    "        if count % 1 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            #indices = np.random.choice(X_train.shape[0], 50, replace=False)\n",
    "            #X_train_tensor = torch.from_numpy(X_train[indices].reshape(-1, seq_dim, input_dim))\n",
    "            #y_pred_valid = model( X_valid_tensor.float())\n",
    "            #val_acc = get_accuracy(y_pred_valid, Y_valid,batch_size=X_valid.shape[0])\n",
    "            \n",
    "            y_pred_train = model( train.float())\n",
    "            train_acc = get_accuracy(y_pred_train,labels,batch_size=labels.shape[0])\n",
    "            \n",
    "            indices = np.random.choice(X_valid.shape[0], 50, replace=False)\n",
    "            \n",
    "            X_valid_tensor = torch.from_numpy(X_valid[indices].reshape(-1, seq_dim, input_dim))\n",
    "            \n",
    "            y_pred_valid = model( X_valid_tensor.float())\n",
    "            val_acc = get_accuracy(y_pred_valid, Y_valid[indices],\n",
    "            batch_size=50)\n",
    "            \n",
    "            '''           \n",
    "            for f in model.parameters():\n",
    "                print('data is')\n",
    "                print(f.data)\n",
    "                print('grad is')\n",
    "                print(f.grad)\n",
    "            '''\n",
    "            \n",
    "            #print('Iteration: {}  Loss: {}' .format(count, loss.data))\n",
    "\n",
    "            print('Iteration: {}  Loss: {}  Train Accuracy: {} Valid Accuracy: {} %'.format(count, loss.data,train_acc,\n",
    "                                                                                            val_acc))\n",
    "            #if(train_acc> 35 and val_acc>35):\n",
    "                #return\n",
    "            '''\n",
    "            # Iterate through test dataset\n",
    "            for signals, labels in valid_loader:\n",
    "                signals = Variable(signals.view(-1, seq_dim, input_dim))\n",
    "                #print(signals.shape)\n",
    "                # Forward propagation\n",
    "                outputs_valid = model(signals.float())\n",
    "\n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs_valid.data, 1)[1]\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum()\n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            train_loss.append(loss.data)\n",
    "            iterations.append(count)\n",
    "            train_acc.append(accuracy)\n",
    "            print('Iteration: {}  Loss: {}  Valid Accuracy: {} %'.format(count, loss.data, accuracy))\n",
    "          '''  \n",
    "                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# In case running this cell gives you an out-of-memory error, you can run the next cell \n",
    "#which breaks up the datasets into chunks to calculate the accuracy\n",
    "\n",
    "X_train_tensor = torch.from_numpy(X_train.reshape(-1, seq_dim, input_dim))\n",
    "print(X_train_tensor.shape)\n",
    "y_pred_train = model( X_train_tensor.float())\n",
    "train_acc = get_accuracy(y_pred_train, Y_train,\n",
    "    batch_size=len(Y_train))\n",
    "print('train accuracy:', train_acc)\n",
    "\n",
    "\n",
    "\n",
    "X_valid_tensor = torch.from_numpy(X_valid.reshape(-1, seq_dim, input_dim))\n",
    "print(X_valid_tensor.shape)\n",
    "y_pred_valid = model( X_valid_tensor.float())\n",
    "val_acc = get_accuracy(y_pred_valid, Y_valid,\n",
    "    batch_size=len(Y_valid))\n",
    "print('validation accuracy:', val_acc)\n",
    "\n",
    "X_test_tensor = torch.from_numpy(X_test.reshape(-1, seq_dim, input_dim))\n",
    "print(X_test_tensor.shape)\n",
    "y_pred_test = model( X_test_tensor.float())\n",
    "test_acc = get_accuracy(y_pred_test, Y_test,\n",
    "    batch_size=len(Y_test))\n",
    "print('test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor = torch.from_numpy(X_test.reshape(-1, seq_dim, input_dim))\n",
    "print(X_test_tensor.shape)\n",
    "y_pred_test = model( X_test_tensor.float())\n",
    "test_acc = get_accuracy(y_pred_test, Y_test,\n",
    "    batch_size=len(Y_test))\n",
    "print('test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the validation accruacy in baaches, since out-of-memory error arises when calculating using the whole set!\n",
    "datset_size =  X_valid.shape[0]\n",
    "size_of_batches = int(datset_size/20)\n",
    "\n",
    "print(datset_size)\n",
    "print(size_of_batches)\n",
    "\n",
    "start = 0\n",
    "\n",
    "valid_accuracies = []\n",
    "for i in range(20):\n",
    "    end = start + size_of_batches\n",
    "    \n",
    "    X_valid_tensor = torch.from_numpy(X_valid[start:end].reshape(-1, seq_dim, input_dim))\n",
    "    print(X_valid_tensor.shape)\n",
    "    y_pred_valid = model( X_valid_tensor.float())\n",
    "    val_acc = get_accuracy(y_pred_valid, Y_valid[start:end],\n",
    "    batch_size=len(Y_valid[start:end]))\n",
    "    start = end\n",
    "    valid_accuracies.append(val_acc)\n",
    "\n",
    "print('validation accuracy:', np.mean(valid_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the test accruacy in baaches, since out-of-memory error arises when calculating using the whole set!\n",
    "datset_size =  X_test.shape[0]\n",
    "size_of_batches = int(datset_size/20)\n",
    "\n",
    "print(datset_size)\n",
    "print(size_of_batches)\n",
    "\n",
    "start = 0\n",
    "\n",
    "test_accuracies = []\n",
    "for i in range(20):\n",
    "    end = start + size_of_batches\n",
    "    \n",
    "    X_test_tensor = torch.from_numpy(X_test[start:end].reshape(-1, seq_dim, input_dim))\n",
    "    print(X_test_tensor.shape)\n",
    "    y_pred_test = model( X_test_tensor.float())\n",
    "    test_acc = get_accuracy(y_pred_test, Y_test[start:end],\n",
    "    batch_size=len(Y_test[start:end]))\n",
    "    start = end\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "print('Test accuracy:', np.mean(test_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating the training accruacy in batches, since out-of-memory error arises when calculating using the whole set!\n",
    "datset_size =  X_train.shape[0]\n",
    "size_of_batches = int(datset_size/50)\n",
    "\n",
    "print(datset_size)\n",
    "print(size_of_batches)\n",
    "\n",
    "start=end = 0\n",
    "\n",
    "train_accuracies = []\n",
    "for i in range(50):\n",
    "    end = start + size_of_batches\n",
    "    \n",
    "    X_train_tensor = torch.from_numpy(X_train[start:end].reshape(-1, seq_dim, input_dim))\n",
    "    print(X_train_tensor.shape)\n",
    "    y_pred_train = model( X_train_tensor .float())\n",
    "    train_acc = get_accuracy(y_pred_train, Y_train[start:end],\n",
    "    batch_size=len(Y_train[start:end]))\n",
    "    start = end\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "print('train accuracy:', np.mean(train_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_accuracy:', np.mean(train_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss vs iterations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(iterations,train_loss)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
