{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for issue loading Utils.preprocess_util\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim  as optim\n",
    "from torch.autograd import Variable\n",
    "from Utils.preprocess_util import *\n",
    "from Utils.visualize import *\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2115, 22, 1000)\n",
      "Cropping trials\n",
      "(177125, 22, 500)\n",
      "After cropping:\n",
      "Training data: (177125, 22, 500)\n",
      "Training target: (177125,)\n",
      "Validation data: (87250, 22, 500)\n",
      "Validation target: (87250,)\n",
      "Test data: (55375, 22, 500)\n",
      "Test target: (55375,)\n",
      "Person train/validation: (2115, 1)\n",
      "Person test: (443, 1)\n",
      "\n",
      "After cropping:\n",
      "Training data: (177125, 22, 500)\n",
      "Training target: (177125,)\n",
      "Validation data: (87250, 22, 500)\n",
      "Validation target: (87250,)\n",
      "Test data: (55375, 22, 500)\n",
      "Test target: (55375,)\n",
      "Person train/validation: (2115, 1)\n",
      "Person test: (443, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train,X_valid,X_test,Y_train,Y_valid,Y_test = load_preprocess_eeg_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        a= x.view(x.size(0), -1)\n",
    "        return a\n",
    "    \n",
    "class threed_to_twod(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)\n",
    "        a = x.reshape(x.shape[0],x.shape[3],x.shape[1])\n",
    "        # print (a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[ 0.0141],\n",
       "          [-0.0128],\n",
       "          [ 0.0456],\n",
       "          ...,\n",
       "          [-0.0340],\n",
       "          [ 0.0013],\n",
       "          [ 0.0537]],\n",
       "\n",
       "         [[-0.0150],\n",
       "          [ 0.0414],\n",
       "          [-0.0109],\n",
       "          ...,\n",
       "          [-0.0090],\n",
       "          [ 0.0045],\n",
       "          [-0.0065]],\n",
       "\n",
       "         [[ 0.0275],\n",
       "          [ 0.0417],\n",
       "          [ 0.0163],\n",
       "          ...,\n",
       "          [-0.0120],\n",
       "          [ 0.0476],\n",
       "          [ 0.0289]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0410],\n",
       "          [ 0.0370],\n",
       "          [-0.0467],\n",
       "          ...,\n",
       "          [ 0.0365],\n",
       "          [ 0.0146],\n",
       "          [ 0.0126]],\n",
       "\n",
       "         [[-0.0332],\n",
       "          [ 0.0429],\n",
       "          [ 0.0522],\n",
       "          ...,\n",
       "          [ 0.0035],\n",
       "          [ 0.0080],\n",
       "          [-0.0157]],\n",
       "\n",
       "         [[ 0.0154],\n",
       "          [-0.0466],\n",
       "          [ 0.0372],\n",
       "          ...,\n",
       "          [ 0.0340],\n",
       "          [-0.0122],\n",
       "          [-0.0518]]],\n",
       "\n",
       "\n",
       "        [[[-0.0456],\n",
       "          [-0.0409],\n",
       "          [-0.0253],\n",
       "          ...,\n",
       "          [ 0.0014],\n",
       "          [ 0.0031],\n",
       "          [-0.0553]],\n",
       "\n",
       "         [[-0.0320],\n",
       "          [ 0.0048],\n",
       "          [ 0.0132],\n",
       "          ...,\n",
       "          [-0.0490],\n",
       "          [ 0.0387],\n",
       "          [ 0.0568]],\n",
       "\n",
       "         [[ 0.0325],\n",
       "          [ 0.0255],\n",
       "          [ 0.0525],\n",
       "          ...,\n",
       "          [-0.0330],\n",
       "          [-0.0252],\n",
       "          [-0.0092]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0089],\n",
       "          [-0.0423],\n",
       "          [-0.0343],\n",
       "          ...,\n",
       "          [ 0.0507],\n",
       "          [ 0.0085],\n",
       "          [ 0.0235]],\n",
       "\n",
       "         [[-0.0092],\n",
       "          [ 0.0177],\n",
       "          [-0.0382],\n",
       "          ...,\n",
       "          [ 0.0171],\n",
       "          [-0.0380],\n",
       "          [-0.0335]],\n",
       "\n",
       "         [[ 0.0413],\n",
       "          [ 0.0362],\n",
       "          [-0.0544],\n",
       "          ...,\n",
       "          [ 0.0273],\n",
       "          [-0.0505],\n",
       "          [ 0.0525]]],\n",
       "\n",
       "\n",
       "        [[[-0.0406],\n",
       "          [-0.0202],\n",
       "          [-0.0509],\n",
       "          ...,\n",
       "          [ 0.0131],\n",
       "          [-0.0339],\n",
       "          [ 0.0581]],\n",
       "\n",
       "         [[-0.0527],\n",
       "          [ 0.0287],\n",
       "          [ 0.0379],\n",
       "          ...,\n",
       "          [-0.0066],\n",
       "          [ 0.0494],\n",
       "          [-0.0228]],\n",
       "\n",
       "         [[ 0.0130],\n",
       "          [-0.0093],\n",
       "          [ 0.0339],\n",
       "          ...,\n",
       "          [-0.0431],\n",
       "          [-0.0503],\n",
       "          [ 0.0558]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0535],\n",
       "          [ 0.0325],\n",
       "          [-0.0336],\n",
       "          ...,\n",
       "          [-0.0113],\n",
       "          [ 0.0035],\n",
       "          [-0.0446]],\n",
       "\n",
       "         [[ 0.0113],\n",
       "          [-0.0248],\n",
       "          [ 0.0012],\n",
       "          ...,\n",
       "          [-0.0377],\n",
       "          [ 0.0563],\n",
       "          [ 0.0536]],\n",
       "\n",
       "         [[-0.0280],\n",
       "          [-0.0560],\n",
       "          [ 0.0432],\n",
       "          ...,\n",
       "          [ 0.0332],\n",
       "          [-0.0572],\n",
       "          [-0.0074]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[ 0.0549],\n",
       "          [ 0.0449],\n",
       "          [-0.0272],\n",
       "          ...,\n",
       "          [ 0.0054],\n",
       "          [ 0.0345],\n",
       "          [-0.0436]],\n",
       "\n",
       "         [[ 0.0027],\n",
       "          [-0.0264],\n",
       "          [ 0.0196],\n",
       "          ...,\n",
       "          [-0.0466],\n",
       "          [ 0.0020],\n",
       "          [-0.0433]],\n",
       "\n",
       "         [[ 0.0320],\n",
       "          [ 0.0478],\n",
       "          [ 0.0075],\n",
       "          ...,\n",
       "          [ 0.0164],\n",
       "          [ 0.0075],\n",
       "          [-0.0532]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-0.0491],\n",
       "          [-0.0116],\n",
       "          [ 0.0223],\n",
       "          ...,\n",
       "          [ 0.0357],\n",
       "          [-0.0280],\n",
       "          [ 0.0062]],\n",
       "\n",
       "         [[ 0.0457],\n",
       "          [-0.0558],\n",
       "          [ 0.0454],\n",
       "          ...,\n",
       "          [-0.0034],\n",
       "          [ 0.0276],\n",
       "          [ 0.0522]],\n",
       "\n",
       "         [[-0.0520],\n",
       "          [-0.0211],\n",
       "          [ 0.0331],\n",
       "          ...,\n",
       "          [ 0.0028],\n",
       "          [-0.0209],\n",
       "          [ 0.0468]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0279],\n",
       "          [ 0.0367],\n",
       "          [ 0.0052],\n",
       "          ...,\n",
       "          [-0.0478],\n",
       "          [ 0.0190],\n",
       "          [-0.0582]],\n",
       "\n",
       "         [[-0.0420],\n",
       "          [-0.0354],\n",
       "          [ 0.0523],\n",
       "          ...,\n",
       "          [-0.0385],\n",
       "          [ 0.0133],\n",
       "          [ 0.0440]],\n",
       "\n",
       "         [[ 0.0401],\n",
       "          [-0.0575],\n",
       "          [-0.0271],\n",
       "          ...,\n",
       "          [ 0.0473],\n",
       "          [-0.0157],\n",
       "          [ 0.0320]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0499],\n",
       "          [-0.0259],\n",
       "          [ 0.0191],\n",
       "          ...,\n",
       "          [ 0.0003],\n",
       "          [ 0.0025],\n",
       "          [ 0.0534]],\n",
       "\n",
       "         [[-0.0458],\n",
       "          [ 0.0466],\n",
       "          [-0.0551],\n",
       "          ...,\n",
       "          [ 0.0213],\n",
       "          [ 0.0566],\n",
       "          [-0.0090]],\n",
       "\n",
       "         [[-0.0378],\n",
       "          [-0.0571],\n",
       "          [ 0.0580],\n",
       "          ...,\n",
       "          [ 0.0293],\n",
       "          [ 0.0341],\n",
       "          [ 0.0255]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0507],\n",
       "          [ 0.0019],\n",
       "          [ 0.0421],\n",
       "          ...,\n",
       "          [-0.0264],\n",
       "          [ 0.0480],\n",
       "          [ 0.0529]],\n",
       "\n",
       "         [[-0.0336],\n",
       "          [-0.0457],\n",
       "          [-0.0016],\n",
       "          ...,\n",
       "          [ 0.0020],\n",
       "          [ 0.0220],\n",
       "          [-0.0475]],\n",
       "\n",
       "         [[-0.0165],\n",
       "          [-0.0050],\n",
       "          [-0.0048],\n",
       "          ...,\n",
       "          [ 0.0572],\n",
       "          [ 0.0088],\n",
       "          [-0.0252]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 0.0221],\n",
       "          [ 0.0268],\n",
       "          [ 0.0453],\n",
       "          ...,\n",
       "          [-0.0509],\n",
       "          [-0.0531],\n",
       "          [-0.0009]],\n",
       "\n",
       "         [[-0.0386],\n",
       "          [ 0.0584],\n",
       "          [ 0.0338],\n",
       "          ...,\n",
       "          [-0.0386],\n",
       "          [ 0.0039],\n",
       "          [ 0.0284]],\n",
       "\n",
       "         [[ 0.0379],\n",
       "          [-0.0234],\n",
       "          [-0.0307],\n",
       "          ...,\n",
       "          [ 0.0210],\n",
       "          [ 0.0486],\n",
       "          [-0.0196]]]], requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('conv_across_time',nn.Conv2d(1,40,kernel_size=(1,51),stride = 1))\n",
    "model.add_module('conv_across_electrodes',nn.Conv2d(40,40,kernel_size=(22,1),stride = 1))\n",
    "model.add_module('BatchNorm2d',nn.BatchNorm2d(40,momentum=0.1))\n",
    "model.add_module('Nonlinearity', nn.ReLU())\n",
    "model.add_module('correct_dimensions',threed_to_twod())\n",
    "model.add_module('AvgPool2d',nn.AvgPool2d(kernel_size=(135,1),stride = (5,1)))\n",
    "model.add_module('drop', nn.Dropout(p=0.5))\n",
    "model.add_module('Flatten',Flatten())    \n",
    "model.add_module('Fc_layer',nn.Linear(2560,10))\n",
    "torch.nn.init.xavier_uniform_(model.conv_across_time.weight, gain=1)\n",
    "torch.nn.init.xavier_uniform_(model.conv_across_electrodes.weight, gain=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.FloatTensor\n",
    "model.type(dtype)\n",
    "loss_fn = nn.CrossEntropyLoss().type(dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N,C,H,W = 18,1,25,1000\n",
    "# x = Variable(torch.tensor(X_train.reshape((18,1, 25, 1000))))\n",
    "x = Variable(torch.tensor(X_train))\n",
    "y = Variable(torch.tensor(Y_train),requires_grad=False)\n",
    "dtype = torch.FloatTensor\n",
    "x.type(dtype)\n",
    "y.type(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor t in range(3):\\n    y_pred = model( x.float())\\n    loss = loss_fn(y_pred,y.type(torch.LongTensor))\\n    print(loss.data)\\n    model.zero_grad()\\n    loss.backward()\\nloss = loss_fn(y_pred,y.type(torch.LongTensor))\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for t in range(3):\n",
    "    y_pred = model( x.float())\n",
    "    loss = loss_fn(y_pred,y.type(torch.LongTensor))\n",
    "    print(loss.data)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "loss = loss_fn(y_pred,y.type(torch.LongTensor))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 88550) loss: 2.335279\n",
      "(Epoch 1 / 50) train acc: 14.000000; val_acc: 18.000000\n",
      "(Iteration 11 / 88550) loss: 1.873507\n",
      "(Iteration 21 / 88550) loss: 1.633958\n",
      "(Iteration 31 / 88550) loss: 1.524756\n",
      "(Iteration 41 / 88550) loss: 1.470626\n",
      "(Iteration 51 / 88550) loss: 1.440654\n",
      "(Iteration 61 / 88550) loss: 1.425282\n",
      "(Iteration 71 / 88550) loss: 1.436383\n",
      "(Iteration 81 / 88550) loss: 1.411892\n",
      "(Iteration 91 / 88550) loss: 1.410616\n",
      "(Iteration 101 / 88550) loss: 1.400183\n",
      "(Iteration 111 / 88550) loss: 1.383582\n",
      "(Iteration 121 / 88550) loss: 1.410469\n",
      "(Iteration 131 / 88550) loss: 1.408475\n",
      "(Iteration 141 / 88550) loss: 1.394894\n",
      "(Iteration 151 / 88550) loss: 1.403960\n",
      "(Iteration 161 / 88550) loss: 1.409156\n",
      "(Iteration 171 / 88550) loss: 1.388676\n",
      "(Iteration 181 / 88550) loss: 1.396498\n",
      "(Iteration 191 / 88550) loss: 1.411703\n",
      "(Iteration 201 / 88550) loss: 1.407684\n",
      "(Iteration 211 / 88550) loss: 1.386379\n",
      "(Iteration 221 / 88550) loss: 1.413196\n",
      "(Iteration 231 / 88550) loss: 1.362272\n",
      "(Iteration 241 / 88550) loss: 1.400559\n",
      "(Iteration 251 / 88550) loss: 1.411157\n",
      "(Iteration 261 / 88550) loss: 1.383344\n",
      "(Iteration 271 / 88550) loss: 1.376414\n",
      "(Iteration 281 / 88550) loss: 1.394278\n",
      "(Iteration 291 / 88550) loss: 1.371836\n",
      "(Iteration 301 / 88550) loss: 1.388660\n",
      "(Iteration 311 / 88550) loss: 1.374370\n",
      "(Iteration 321 / 88550) loss: 1.363950\n",
      "(Iteration 331 / 88550) loss: 1.378623\n",
      "(Iteration 341 / 88550) loss: 1.363946\n",
      "(Iteration 351 / 88550) loss: 1.351609\n",
      "(Iteration 361 / 88550) loss: 1.366402\n",
      "(Iteration 371 / 88550) loss: 1.357452\n",
      "(Iteration 381 / 88550) loss: 1.348272\n",
      "(Iteration 391 / 88550) loss: 1.361998\n",
      "(Iteration 401 / 88550) loss: 1.331991\n",
      "(Iteration 411 / 88550) loss: 1.313369\n",
      "(Iteration 421 / 88550) loss: 1.348807\n",
      "(Iteration 431 / 88550) loss: 1.365806\n",
      "(Iteration 441 / 88550) loss: 1.355088\n",
      "(Iteration 451 / 88550) loss: 1.326253\n",
      "(Iteration 461 / 88550) loss: 1.346252\n",
      "(Iteration 471 / 88550) loss: 1.298403\n",
      "(Iteration 481 / 88550) loss: 1.363960\n",
      "(Iteration 491 / 88550) loss: 1.356946\n",
      "(Iteration 501 / 88550) loss: 1.344088\n",
      "(Iteration 511 / 88550) loss: 1.320912\n",
      "(Iteration 521 / 88550) loss: 1.338924\n",
      "(Iteration 531 / 88550) loss: 1.343677\n",
      "(Iteration 541 / 88550) loss: 1.292924\n",
      "(Iteration 551 / 88550) loss: 1.293765\n",
      "(Iteration 561 / 88550) loss: 1.249105\n",
      "(Iteration 571 / 88550) loss: 1.376532\n",
      "(Iteration 581 / 88550) loss: 1.281705\n",
      "(Iteration 591 / 88550) loss: 1.360108\n",
      "(Iteration 601 / 88550) loss: 1.315302\n",
      "(Iteration 611 / 88550) loss: 1.277258\n",
      "(Iteration 621 / 88550) loss: 1.333317\n",
      "(Iteration 631 / 88550) loss: 1.307705\n",
      "(Iteration 641 / 88550) loss: 1.269065\n",
      "(Iteration 651 / 88550) loss: 1.262997\n",
      "(Iteration 661 / 88550) loss: 1.270547\n",
      "(Iteration 671 / 88550) loss: 1.280945\n",
      "(Iteration 681 / 88550) loss: 1.268419\n",
      "(Iteration 691 / 88550) loss: 1.229123\n",
      "(Iteration 701 / 88550) loss: 1.289456\n",
      "(Iteration 711 / 88550) loss: 1.331215\n",
      "(Iteration 721 / 88550) loss: 1.288023\n",
      "(Iteration 731 / 88550) loss: 1.299167\n",
      "(Iteration 741 / 88550) loss: 1.256123\n",
      "(Iteration 751 / 88550) loss: 1.298492\n",
      "(Iteration 761 / 88550) loss: 1.205964\n",
      "(Iteration 771 / 88550) loss: 1.250433\n",
      "(Iteration 781 / 88550) loss: 1.302960\n",
      "(Iteration 791 / 88550) loss: 1.359422\n",
      "(Iteration 801 / 88550) loss: 1.303538\n",
      "(Iteration 811 / 88550) loss: 1.253342\n",
      "(Iteration 821 / 88550) loss: 1.215951\n",
      "(Iteration 831 / 88550) loss: 1.276210\n",
      "(Iteration 841 / 88550) loss: 1.327599\n",
      "(Iteration 851 / 88550) loss: 1.303015\n",
      "(Iteration 861 / 88550) loss: 1.221984\n",
      "(Iteration 871 / 88550) loss: 1.291173\n",
      "(Iteration 881 / 88550) loss: 1.258197\n",
      "(Iteration 891 / 88550) loss: 1.244468\n",
      "(Iteration 901 / 88550) loss: 1.267596\n",
      "(Iteration 911 / 88550) loss: 1.211446\n",
      "(Iteration 921 / 88550) loss: 1.346521\n",
      "(Iteration 931 / 88550) loss: 1.213170\n",
      "(Iteration 941 / 88550) loss: 1.320821\n",
      "(Iteration 951 / 88550) loss: 1.260148\n",
      "(Iteration 961 / 88550) loss: 1.184798\n",
      "(Iteration 971 / 88550) loss: 1.204342\n",
      "(Iteration 981 / 88550) loss: 1.269402\n",
      "(Iteration 991 / 88550) loss: 1.206398\n",
      "(Iteration 1001 / 88550) loss: 1.225763\n",
      "(Iteration 1011 / 88550) loss: 1.205773\n",
      "(Iteration 1021 / 88550) loss: 1.256655\n",
      "(Iteration 1031 / 88550) loss: 1.256528\n",
      "(Iteration 1041 / 88550) loss: 1.288440\n",
      "(Iteration 1051 / 88550) loss: 1.217097\n",
      "(Iteration 1061 / 88550) loss: 1.211969\n",
      "(Iteration 1071 / 88550) loss: 1.278463\n",
      "(Iteration 1081 / 88550) loss: 1.237288\n",
      "(Iteration 1091 / 88550) loss: 1.208920\n",
      "(Iteration 1101 / 88550) loss: 1.261031\n",
      "(Iteration 1111 / 88550) loss: 1.220005\n",
      "(Iteration 1121 / 88550) loss: 1.315719\n",
      "(Iteration 1131 / 88550) loss: 1.202606\n",
      "(Iteration 1141 / 88550) loss: 1.272426\n",
      "(Iteration 1151 / 88550) loss: 1.153494\n",
      "(Iteration 1161 / 88550) loss: 1.148475\n",
      "(Iteration 1171 / 88550) loss: 1.155368\n",
      "(Iteration 1181 / 88550) loss: 1.313072\n",
      "(Iteration 1191 / 88550) loss: 1.204060\n",
      "(Iteration 1201 / 88550) loss: 1.195486\n",
      "(Iteration 1211 / 88550) loss: 1.201456\n",
      "(Iteration 1221 / 88550) loss: 1.228167\n",
      "(Iteration 1231 / 88550) loss: 1.212933\n",
      "(Iteration 1241 / 88550) loss: 1.178902\n",
      "(Iteration 1251 / 88550) loss: 1.201290\n",
      "(Iteration 1261 / 88550) loss: 1.180779\n",
      "(Iteration 1271 / 88550) loss: 1.169843\n",
      "(Iteration 1281 / 88550) loss: 1.245553\n",
      "(Iteration 1291 / 88550) loss: 1.195662\n",
      "(Iteration 1301 / 88550) loss: 1.279476\n",
      "(Iteration 1311 / 88550) loss: 1.216923\n",
      "(Iteration 1321 / 88550) loss: 1.184020\n",
      "(Iteration 1331 / 88550) loss: 1.288390\n",
      "(Iteration 1341 / 88550) loss: 1.201846\n",
      "(Iteration 1351 / 88550) loss: 1.175545\n",
      "(Iteration 1361 / 88550) loss: 1.196691\n",
      "(Iteration 1371 / 88550) loss: 1.245603\n",
      "(Iteration 1381 / 88550) loss: 1.171514\n",
      "(Iteration 1391 / 88550) loss: 1.249336\n",
      "(Iteration 1401 / 88550) loss: 1.210410\n",
      "(Iteration 1411 / 88550) loss: 1.200251\n",
      "(Iteration 1421 / 88550) loss: 1.241593\n",
      "(Iteration 1431 / 88550) loss: 1.175510\n",
      "(Iteration 1441 / 88550) loss: 1.189537\n",
      "(Iteration 1451 / 88550) loss: 1.243737\n",
      "(Iteration 1461 / 88550) loss: 1.244919\n",
      "(Iteration 1471 / 88550) loss: 1.212214\n",
      "(Iteration 1481 / 88550) loss: 1.190681\n",
      "(Iteration 1491 / 88550) loss: 1.291314\n",
      "(Iteration 1501 / 88550) loss: 1.135884\n",
      "(Iteration 1511 / 88550) loss: 1.179722\n",
      "(Iteration 1521 / 88550) loss: 1.250827\n",
      "(Iteration 1531 / 88550) loss: 1.160655\n",
      "(Iteration 1541 / 88550) loss: 1.204068\n",
      "(Iteration 1551 / 88550) loss: 1.123785\n",
      "(Iteration 1561 / 88550) loss: 1.135229\n",
      "(Iteration 1571 / 88550) loss: 1.143709\n",
      "(Iteration 1581 / 88550) loss: 1.307243\n",
      "(Iteration 1591 / 88550) loss: 1.112466\n",
      "(Iteration 1601 / 88550) loss: 1.174704\n",
      "(Iteration 1611 / 88550) loss: 1.186817\n",
      "(Iteration 1621 / 88550) loss: 1.168179\n",
      "(Iteration 1631 / 88550) loss: 1.104771\n",
      "(Iteration 1641 / 88550) loss: 1.145365\n",
      "(Iteration 1651 / 88550) loss: 1.228845\n",
      "(Iteration 1661 / 88550) loss: 1.152844\n",
      "(Iteration 1671 / 88550) loss: 1.244901\n",
      "(Iteration 1681 / 88550) loss: 1.090256\n",
      "(Iteration 1691 / 88550) loss: 1.171533\n",
      "(Iteration 1701 / 88550) loss: 1.276250\n",
      "(Iteration 1711 / 88550) loss: 1.271492\n",
      "(Iteration 1721 / 88550) loss: 1.135975\n",
      "(Iteration 1731 / 88550) loss: 1.180841\n",
      "(Iteration 1741 / 88550) loss: 1.161660\n",
      "(Iteration 1751 / 88550) loss: 1.179757\n",
      "(Iteration 1761 / 88550) loss: 1.157086\n",
      "(Iteration 1771 / 88550) loss: 1.164163\n",
      "(Epoch 2 / 50) train acc: 88.000000; val_acc: 22.000000\n",
      "(Iteration 1781 / 88550) loss: 1.135998\n",
      "(Iteration 1791 / 88550) loss: 1.174760\n",
      "(Iteration 1801 / 88550) loss: 1.126484\n",
      "(Iteration 1811 / 88550) loss: 1.117802\n",
      "(Iteration 1821 / 88550) loss: 1.072709\n",
      "(Iteration 1831 / 88550) loss: 1.180614\n",
      "(Iteration 1841 / 88550) loss: 1.209956\n",
      "(Iteration 1851 / 88550) loss: 1.120281\n",
      "(Iteration 1861 / 88550) loss: 1.095568\n",
      "(Iteration 1871 / 88550) loss: 1.132957\n",
      "(Iteration 1881 / 88550) loss: 1.116138\n",
      "(Iteration 1891 / 88550) loss: 1.141922\n",
      "(Iteration 1901 / 88550) loss: 1.126303\n",
      "(Iteration 1911 / 88550) loss: 1.079696\n",
      "(Iteration 1921 / 88550) loss: 1.104875\n",
      "(Iteration 1931 / 88550) loss: 1.122184\n",
      "(Iteration 1941 / 88550) loss: 1.149379\n",
      "(Iteration 1951 / 88550) loss: 1.160622\n",
      "(Iteration 1961 / 88550) loss: 1.160157\n",
      "(Iteration 1971 / 88550) loss: 1.199080\n",
      "(Iteration 1981 / 88550) loss: 1.144128\n",
      "(Iteration 1991 / 88550) loss: 1.130967\n",
      "(Iteration 2001 / 88550) loss: 1.130946\n",
      "(Iteration 2011 / 88550) loss: 1.124020\n",
      "(Iteration 2021 / 88550) loss: 1.127846\n",
      "(Iteration 2031 / 88550) loss: 1.154445\n",
      "(Iteration 2041 / 88550) loss: 1.167790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 2051 / 88550) loss: 1.119657\n",
      "(Iteration 2061 / 88550) loss: 1.203485\n",
      "(Iteration 2071 / 88550) loss: 1.146266\n",
      "(Iteration 2081 / 88550) loss: 1.138059\n",
      "(Iteration 2091 / 88550) loss: 1.150768\n",
      "(Iteration 2101 / 88550) loss: 1.112139\n",
      "(Iteration 2111 / 88550) loss: 1.246979\n",
      "(Iteration 2121 / 88550) loss: 1.115684\n",
      "(Iteration 2131 / 88550) loss: 1.040018\n",
      "(Iteration 2141 / 88550) loss: 1.195043\n",
      "(Iteration 2151 / 88550) loss: 1.178815\n",
      "(Iteration 2161 / 88550) loss: 1.242809\n",
      "(Iteration 2171 / 88550) loss: 1.100996\n",
      "(Iteration 2181 / 88550) loss: 1.144193\n",
      "(Iteration 2191 / 88550) loss: 1.088432\n",
      "(Iteration 2201 / 88550) loss: 1.139265\n",
      "(Iteration 2211 / 88550) loss: 1.112007\n",
      "(Iteration 2221 / 88550) loss: 1.109902\n",
      "(Iteration 2231 / 88550) loss: 1.142923\n",
      "(Iteration 2241 / 88550) loss: 1.121666\n",
      "(Iteration 2251 / 88550) loss: 1.130620\n",
      "(Iteration 2261 / 88550) loss: 1.074307\n",
      "(Iteration 2271 / 88550) loss: 1.057264\n",
      "(Iteration 2281 / 88550) loss: 1.114834\n",
      "(Iteration 2291 / 88550) loss: 1.126265\n",
      "(Iteration 2301 / 88550) loss: 1.089840\n",
      "(Iteration 2311 / 88550) loss: 1.028080\n",
      "(Iteration 2321 / 88550) loss: 1.096474\n",
      "(Iteration 2331 / 88550) loss: 1.103466\n",
      "(Iteration 2341 / 88550) loss: 1.137594\n",
      "(Iteration 2351 / 88550) loss: 1.147116\n",
      "(Iteration 2361 / 88550) loss: 1.063108\n",
      "(Iteration 2371 / 88550) loss: 1.156503\n",
      "(Iteration 2381 / 88550) loss: 1.158182\n",
      "(Iteration 2391 / 88550) loss: 1.070426\n",
      "(Iteration 2401 / 88550) loss: 1.070490\n",
      "(Iteration 2411 / 88550) loss: 1.114994\n",
      "(Iteration 2421 / 88550) loss: 1.151450\n",
      "(Iteration 2431 / 88550) loss: 1.137329\n",
      "(Iteration 2441 / 88550) loss: 1.141714\n",
      "(Iteration 2451 / 88550) loss: 1.091983\n",
      "(Iteration 2461 / 88550) loss: 1.185693\n",
      "(Iteration 2471 / 88550) loss: 1.043840\n",
      "(Iteration 2481 / 88550) loss: 1.167878\n",
      "(Iteration 2491 / 88550) loss: 1.053717\n",
      "(Iteration 2501 / 88550) loss: 1.007457\n",
      "(Iteration 2511 / 88550) loss: 1.102125\n",
      "(Iteration 2521 / 88550) loss: 1.064224\n",
      "(Iteration 2531 / 88550) loss: 1.110426\n",
      "(Iteration 2541 / 88550) loss: 1.071477\n",
      "(Iteration 2551 / 88550) loss: 1.124511\n",
      "(Iteration 2561 / 88550) loss: 1.092966\n",
      "(Iteration 2571 / 88550) loss: 1.106513\n",
      "(Iteration 2581 / 88550) loss: 1.079715\n",
      "(Iteration 2591 / 88550) loss: 1.008801\n",
      "(Iteration 2601 / 88550) loss: 1.150184\n",
      "(Iteration 2611 / 88550) loss: 1.078573\n",
      "(Iteration 2621 / 88550) loss: 1.095024\n",
      "(Iteration 2631 / 88550) loss: 1.075292\n",
      "(Iteration 2641 / 88550) loss: 1.126711\n",
      "(Iteration 2651 / 88550) loss: 1.054041\n",
      "(Iteration 2661 / 88550) loss: 1.080507\n",
      "(Iteration 2671 / 88550) loss: 1.092212\n",
      "(Iteration 2681 / 88550) loss: 1.078987\n",
      "(Iteration 2691 / 88550) loss: 1.068187\n",
      "(Iteration 2701 / 88550) loss: 1.156854\n",
      "(Iteration 2711 / 88550) loss: 1.022911\n",
      "(Iteration 2721 / 88550) loss: 1.102676\n",
      "(Iteration 2731 / 88550) loss: 1.098256\n",
      "(Iteration 2741 / 88550) loss: 1.111243\n",
      "(Iteration 2751 / 88550) loss: 1.168741\n",
      "(Iteration 2761 / 88550) loss: 1.124689\n",
      "(Iteration 2771 / 88550) loss: 0.941533\n",
      "(Iteration 2781 / 88550) loss: 1.120181\n",
      "(Iteration 2791 / 88550) loss: 1.073971\n",
      "(Iteration 2801 / 88550) loss: 1.024349\n",
      "(Iteration 2811 / 88550) loss: 1.130184\n",
      "(Iteration 2821 / 88550) loss: 1.047871\n",
      "(Iteration 2831 / 88550) loss: 1.131012\n",
      "(Iteration 2841 / 88550) loss: 1.082161\n",
      "(Iteration 2851 / 88550) loss: 1.219302\n",
      "(Iteration 2861 / 88550) loss: 1.044788\n",
      "(Iteration 2871 / 88550) loss: 0.984383\n",
      "(Iteration 2881 / 88550) loss: 1.071346\n",
      "(Iteration 2891 / 88550) loss: 1.023656\n",
      "(Iteration 2901 / 88550) loss: 1.061419\n",
      "(Iteration 2911 / 88550) loss: 1.146721\n",
      "(Iteration 2921 / 88550) loss: 1.013351\n",
      "(Iteration 2931 / 88550) loss: 1.078392\n",
      "(Iteration 2941 / 88550) loss: 1.135023\n",
      "(Iteration 2951 / 88550) loss: 1.179330\n",
      "(Iteration 2961 / 88550) loss: 1.091803\n",
      "(Iteration 2971 / 88550) loss: 1.091699\n",
      "(Iteration 2981 / 88550) loss: 1.101525\n",
      "(Iteration 2991 / 88550) loss: 1.057451\n",
      "(Iteration 3001 / 88550) loss: 1.073467\n",
      "(Iteration 3011 / 88550) loss: 1.071165\n",
      "(Iteration 3021 / 88550) loss: 1.030015\n",
      "(Iteration 3031 / 88550) loss: 1.235501\n",
      "(Iteration 3041 / 88550) loss: 1.046811\n",
      "(Iteration 3051 / 88550) loss: 1.040637\n",
      "(Iteration 3061 / 88550) loss: 1.053714\n",
      "(Iteration 3071 / 88550) loss: 1.026020\n",
      "(Iteration 3081 / 88550) loss: 1.164831\n",
      "(Iteration 3091 / 88550) loss: 1.034898\n",
      "(Iteration 3101 / 88550) loss: 1.126845\n",
      "(Iteration 3111 / 88550) loss: 1.101691\n",
      "(Iteration 3121 / 88550) loss: 1.024674\n",
      "(Iteration 3131 / 88550) loss: 1.062338\n",
      "(Iteration 3141 / 88550) loss: 1.007791\n",
      "(Iteration 3151 / 88550) loss: 1.037762\n",
      "(Iteration 3161 / 88550) loss: 1.082236\n",
      "(Iteration 3171 / 88550) loss: 1.022920\n",
      "(Iteration 3181 / 88550) loss: 1.027877\n",
      "(Iteration 3191 / 88550) loss: 1.048530\n",
      "(Iteration 3201 / 88550) loss: 1.118408\n",
      "(Iteration 3211 / 88550) loss: 1.051502\n",
      "(Iteration 3221 / 88550) loss: 1.105621\n",
      "(Iteration 3231 / 88550) loss: 1.005115\n",
      "(Iteration 3241 / 88550) loss: 1.059762\n",
      "(Iteration 3251 / 88550) loss: 1.023026\n",
      "(Iteration 3261 / 88550) loss: 1.022235\n",
      "(Iteration 3271 / 88550) loss: 1.142851\n",
      "(Iteration 3281 / 88550) loss: 1.062649\n",
      "(Iteration 3291 / 88550) loss: 1.053794\n",
      "(Iteration 3301 / 88550) loss: 1.012434\n",
      "(Iteration 3311 / 88550) loss: 1.030202\n",
      "(Iteration 3321 / 88550) loss: 1.011429\n",
      "(Iteration 3331 / 88550) loss: 1.118323\n",
      "(Iteration 3341 / 88550) loss: 1.108563\n",
      "(Iteration 3351 / 88550) loss: 1.034549\n",
      "(Iteration 3361 / 88550) loss: 0.994289\n",
      "(Iteration 3371 / 88550) loss: 1.041326\n",
      "(Iteration 3381 / 88550) loss: 1.022516\n",
      "(Iteration 3391 / 88550) loss: 1.125610\n",
      "(Iteration 3401 / 88550) loss: 1.191963\n",
      "(Iteration 3411 / 88550) loss: 1.111552\n",
      "(Iteration 3421 / 88550) loss: 1.010673\n",
      "(Iteration 3431 / 88550) loss: 1.059829\n",
      "(Iteration 3441 / 88550) loss: 1.145590\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-463093d0f94f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \"\"\"\n\u001b[1;32m--> 102\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_train = X_train.shape[0]\n",
    "num_valid = X_valid.shape[0]\n",
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "iterations_per_epoch = max(num_train // batch_size, 1)\n",
    "num_iterations = num_epochs * iterations_per_epoch\n",
    "epoch = 1\n",
    "train_loss = []\n",
    "iterations = []\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)   \n",
    "for t in range(num_iterations):\n",
    "    batch_mask = np.random.choice(num_train, batch_size)\n",
    "    X_batch = X_train[batch_mask]\n",
    "    y_batch = Y_train[batch_mask]\n",
    "    X_batch_tensor = threeD_to_fourDTensor(X_batch)\n",
    "    y_batch_tensor = Variable(torch.tensor(y_batch))\n",
    "    \n",
    "    y_pred = model( X_batch_tensor.float())\n",
    "    \n",
    "    loss = loss_fn(y_pred,y_batch_tensor.type(torch.LongTensor))\n",
    "    \n",
    "    train_loss.append(loss.data)\n",
    "    iterations.append(t)\n",
    "        \n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if(t%10 == 0):\n",
    "        print('(Iteration %d / %d) loss: %f' % (\n",
    "                       t + 1, num_iterations, loss.detach().numpy()))\n",
    "    \n",
    "    epoch_end = (t + 1) % iterations_per_epoch == 0\n",
    "    \n",
    "    if epoch_end:\n",
    "                epoch += 1\n",
    "    \n",
    "    first_it = (t == 0)\n",
    "    last_it = (t == num_iterations - 1)\n",
    "\n",
    "    if first_it or last_it or epoch_end:\n",
    "        X_train_tensor =threeD_to_fourDTensor(X_train[0:100,:,:])\n",
    "        y_pred_train = model( X_train_tensor.float())\n",
    "        train_acc = get_accuracy(y_pred_train, Y_train[0:100],\n",
    "            batch_size=50)\n",
    "        \n",
    "        X_valid_tensor = threeD_to_fourDTensor(X_valid[0:50,:,:])\n",
    "        y_pred_valid = model( X_valid_tensor.float())\n",
    "        val_acc = get_accuracy(y_pred_valid, Y_valid[0:50],\n",
    "            batch_size=50)\n",
    "        print('(Epoch %d / %d) train acc: %f; val_acc: %f' % (\n",
    "                           epoch, num_epochs, train_acc, val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 3GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-b08c0ba82857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred_valid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mthreeD_to_fourDTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m val_acc = get_accuracy(y_pred_valid, Y_valid,\n\u001b[0;32m      4\u001b[0m     batch_size=X_valid.shape[0])\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'validation accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 3GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred_valid = model( threeD_to_fourDTensor(X_valid).float())\n",
    "val_acc = get_accuracy(y_pred_valid, Y_valid,\n",
    "    batch_size=X_valid.shape[0])\n",
    "print('validation accuracy:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87250\n",
      "174\n",
      "validation accuracy: 37.1183908045977\n"
     ]
    }
   ],
   "source": [
    "#Calculating the validation accruacy in baaches, since out-of-memory error arises when calculating using the whole set!\n",
    "datset_size =  X_valid.shape[0]\n",
    "size_of_batches = int(datset_size/500)\n",
    "\n",
    "print(datset_size)\n",
    "print(size_of_batches)\n",
    "\n",
    "start = 0\n",
    "\n",
    "valid_accuracies = []\n",
    "for i in range(500):\n",
    "    end = start + size_of_batches\n",
    "    \n",
    "    y_pred_valid = model( threeD_to_fourDTensor(X_valid[start:end]).float())\n",
    "    val_acc = get_accuracy(y_pred_valid, Y_valid[start:end],\n",
    "    batch_size=len(Y_valid[start:end]))\n",
    "    start = end\n",
    "    valid_accuracies.append(val_acc)\n",
    "\n",
    "print('validation accuracy:', np.mean(valid_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177125\n",
      "354\n",
      "train accuracy: 47.509604519774015\n"
     ]
    }
   ],
   "source": [
    "#Calculating the training accruacy in batches, since out-of-memory error arises when calculating using the whole set!\n",
    "datset_size =  X_train.shape[0]\n",
    "size_of_batches = int(datset_size/500)\n",
    "\n",
    "print(datset_size)\n",
    "print(size_of_batches)\n",
    "\n",
    "start=end = 0\n",
    "\n",
    "train_accuracies = []\n",
    "for i in range(500):\n",
    "    end = start + size_of_batches\n",
    "    \n",
    "    y_pred_train = model( threeD_to_fourDTensor(X_train[start:end]).float())\n",
    "    train_acc = get_accuracy(y_pred_train, Y_train[start:end],\n",
    "    batch_size=len(Y_train[start:end]))\n",
    "    start = end\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "print('train accuracy:', np.mean(train_accuracies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55375\n",
      "110\n",
      "Test accuracy: 35.74\n"
     ]
    }
   ],
   "source": [
    "#Calculating the test accruacy in baaches, since out-of-memory error arises when calculating using the whole set!\n",
    "datset_size =  X_test.shape[0]\n",
    "size_of_batches = int(datset_size/500)\n",
    "\n",
    "print(datset_size)\n",
    "print(size_of_batches)\n",
    "\n",
    "start = 0\n",
    "\n",
    "test_accuracies = []\n",
    "for i in range(500):\n",
    "    end = start + size_of_batches\n",
    "    y_pred_test = model( threeD_to_fourDTensor(X_test[start:end]).float())\n",
    "    test_acc = get_accuracy(y_pred_test, Y_test[start:end],\n",
    "    batch_size=len(Y_test[start:end]))\n",
    "    start = end\n",
    "    test_accuracies.append(test_acc)\n",
    "\n",
    "print('Test accuracy:', np.mean(test_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 11GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3bd85026ccb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX_test_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreeD_to_fourDTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my_pred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mX_test_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m test_acc = get_accuracy(y_pred_test, Y_test,\n\u001b[0;32m      4\u001b[0m     batch_size=X_test.shape[0])\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test accuracy:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\nn-hw4\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[1;32m--> 320\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 11GB. Buy new RAM! at ..\\aten\\src\\TH\\THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "X_test_tensor = threeD_to_fourDTensor(X_test)\n",
    "y_pred_test = model( X_test_tensor.float())\n",
    "test_acc = get_accuracy(y_pred_test, Y_test,\n",
    "    batch_size=X_test.shape[0])\n",
    "print('test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(iterations,train_loss)\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
